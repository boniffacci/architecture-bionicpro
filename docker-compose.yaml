services:
  keycloak-db:
    container_name: keycloak-db
    image: postgres:14
    environment:
      POSTGRES_DB: keycloak_db
      POSTGRES_USER: keycloak_user
      POSTGRES_PASSWORD: keycloak_password
#    volumes:
#      - ./postgres-keycloak-data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
  keycloak:
    container_name: keycloak
    image: quay.io/keycloak/keycloak:26.4
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak-db:5432/keycloak_db
      KC_DB_USERNAME: keycloak_user
      KC_DB_PASSWORD: keycloak_password
      # Frontend URL для публичного доступа (используется как issuer в токенах)
      KC_HOSTNAME_URL: http://localhost:8080
      KC_HOSTNAME_ADMIN_URL: http://localhost:8080
      # Отключаем строгую проверку hostname для dev-режима
      KC_HOSTNAME_STRICT: "false"
    command: 
      - start-dev
      - --hostname=http://localhost:8080  # frontend URL
      - --import-realm
    volumes:
      - ./keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "timeout 3 bash -c 'exec 3<>/dev/tcp/localhost/8080'" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 20s
    depends_on:
      - keycloak-db
  openldap-zambia:
    container_name: openldap-zambia
    image: osixia/openldap:1.5.0
    environment:
      LDAP_ORGANISATION: "Zambia Company"
      LDAP_DOMAIN: "zambia.local"
      LDAP_ADMIN_PASSWORD: "admin123"
      LDAP_TLS: "false"
      LDAP_SEED_INTERNAL_LDIF_PATH: "/seed-ldif"
    volumes:
#      - ./ldap/data:/var/lib/ldap
#      - ./ldap/config:/etc/ldap/slapd.d
      - ./ldap:/seed-ldif
    ports:
      - "389:389"
  phpldapadmin:
    container_name: phpldapadmin
    image: osixia/phpldapadmin:latest
    environment:
      PHPLDAPADMIN_LDAP_HOSTS: openldap-zambia
      PHPLDAPADMIN_HTTPS: "false"
    ports:
      - "8081:80"
    depends_on:
      - openldap-zambia
  redis:
    # Redis для хранения сессий auth_proxy
    container_name: redis
    image: redis:7-alpine
    ports:
      - "6379:6379"  # Порт Redis
    command: redis-server --appendonly yes  # Включаем persistence
    # volumes:
    #   - ./redis-data:/data  # Персистентное хранилище (раскомментировать при необходимости)
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]  # Проверка здоровья Redis
      interval: 5s
      timeout: 3s
      retries: 5
#  frontend:
#    build:
#      context: ./frontend
#      dockerfile: Dockerfile
#    ports:
#      - "3000:3000"
#    environment:
#      REACT_APP_API_URL: http://localhost:8000
#      REACT_APP_KEYCLOAK_URL: http://localhost:8080
#      REACT_APP_KEYCLOAK_REALM: reports-realm
#      REACT_APP_KEYCLOAK_CLIENT_ID: reports-frontend

  crm-db:
    # PostgreSQL для CRM-системы и интернет-магазина (с поддержкой репликации для Debezium)
    container_name: crm-db
    image: postgres:14
    environment:
      POSTGRES_DB: crm_db  # Имя базы данных
      POSTGRES_USER: crm_user  # Пользователь БД
      POSTGRES_PASSWORD: crm_password  # Пароль пользователя
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"  # Включаем логическую репликацию для Debezium
#    volumes:
#        - ./postgres-crm-data:/var/lib/postgresql/data  # Персистентное хранилище данных
    ports:
      - "5444:5432"  # Порт для подключения к CRM DB
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U crm_user -d crm_db"]  # Проверка готовности PostgreSQL
      interval: 5s
      timeout: 3s
      retries: 5
  
  telemetry-db:
    # PostgreSQL для хранения телеметрии с бионических протезов (с поддержкой репликации для Debezium)
    container_name: telemetry-db
    image: postgres:14
    environment:
      POSTGRES_DB: telemetry_db  # Имя базы данных
      POSTGRES_USER: telemetry_user  # Пользователь БД
      POSTGRES_PASSWORD: telemetry_password  # Пароль пользователя
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"  # Включаем логическую репликацию для Debezium
#    volumes:
#        - ./postgres-telemetry-data:/var/lib/postgresql/data  # Персистентное хранилище данных
    ports:
      - "5445:5432"  # Порт для подключения к Telemetry DB
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U telemetry_user -d telemetry_db"]  # Проверка готовности PostgreSQL
      interval: 5s
      timeout: 3s
      retries: 5
  crm-api:
    # Микросервис CRM API для регистрации пользователей
    container_name: crm-api
    build:
      context: ./crm_api  # Папка с Dockerfile и кодом микросервиса
      dockerfile: Dockerfile  # Dockerfile для сборки образа
    depends_on:
      crm-db:
        condition: service_healthy  # Ждём готовности CRM DB
    environment:
      # Переменные окружения для подключения к БД (внутри Docker-сети используется имя сервиса)
      DB_HOST: crm-db  # Хост базы данных (имя сервиса в Docker Compose)
      DB_PORT: 5432  # Порт внутри контейнера PostgreSQL
      DB_NAME: crm_db  # Имя базы данных
      DB_USER: crm_user  # Пользователь БД
      DB_PASSWORD: crm_password  # Пароль пользователя
    ports:
      - "3001:3001"  # Порт для доступа к CRM API с хоста
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3001/health || exit 1"]  # Проверка работоспособности API
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s
  
  telemetry-api:
    # Микросервис Telemetry API для сбора телеметрии с бионических протезов
    container_name: telemetry-api
    build:
      context: ./telemetry_api  # Папка с Dockerfile и кодом микросервиса
      dockerfile: Dockerfile  # Dockerfile для сборки образа
    depends_on:
      telemetry-db:
        condition: service_healthy  # Ждём готовности Telemetry DB
    environment:
      # Переменные окружения для подключения к БД (внутри Docker-сети используется имя сервиса)
      DB_HOST: telemetry-db  # Хост базы данных (имя сервиса в Docker Compose)
      DB_PORT: 5432  # Порт внутри контейнера PostgreSQL
      DB_NAME: telemetry_db  # Имя базы данных
      DB_USER: telemetry_user  # Пользователь БД
      DB_PASSWORD: telemetry_password  # Пароль пользователя
    ports:
      - "3002:3002"  # Порт для доступа к Telemetry API с хоста
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3002/health || exit 1"]  # Проверка работоспособности API
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s
  
  reports-api:
    # Микросервис Reports API для генерации отчетов на основе данных из ClickHouse
    container_name: reports-api
    build:
      context: ./reports_api  # Папка с Dockerfile и кодом микросервиса
      dockerfile: Dockerfile  # Dockerfile для сборки образа
    depends_on:
      keycloak:
        condition: service_healthy  # Ждём готовности Keycloak
      minio:
        condition: service_started  # Ждём запуска MinIO
      olap-db:
        condition: service_healthy  # Ждём готовности ClickHouse
      crm-db:
        condition: service_healthy  # Ждём готовности CRM DB для импорта данных
      telemetry-db:
        condition: service_healthy  # Ждём готовности Telemetry DB для импорта данных
    environment:
      # Переменные окружения для подключения к сервисам (внутри Docker-сети используются имена сервисов)
      KEYCLOAK_URL: http://keycloak:8080  # URL Keycloak внутри Docker-сети
      CLICKHOUSE_HOST: olap-db  # Хост ClickHouse (имя сервиса в Docker Compose)
      CLICKHOUSE_PORT: 8123  # HTTP-порт ClickHouse
      CLICKHOUSE_USER: default  # Пользователь ClickHouse
      CLICKHOUSE_PASSWORD: clickhouse_password  # Пароль ClickHouse
      MINIO_HOST: minio:9000  # Хост MinIO (имя сервиса в Docker Compose)
      MINIO_ACCESS_KEY: minio_user  # Access key MinIO
      MINIO_SECRET_KEY: minio_password  # Secret key MinIO
      KAFKA_BROKER: kafka:9092  # Адрес Kafka-брокера
      # Переменные для подключения к CRM DB (для импорта данных)
      CRM_DB_HOST: crm-db
      CRM_DB_PORT: 5432
      CRM_DB_NAME: crm_db
      CRM_DB_USER: crm_user
      CRM_DB_PASSWORD: crm_password
      # Переменные для подключения к Telemetry DB (для импорта данных)
      TELEMETRY_DB_HOST: telemetry-db
      TELEMETRY_DB_PORT: 5432
      TELEMETRY_DB_NAME: telemetry_db
      TELEMETRY_DB_USER: telemetry_user
      TELEMETRY_DB_PASSWORD: telemetry_password
    ports:
      - "3003:3003"  # Порт для доступа к Reports API с хоста
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3003/ || exit 1"]  # Проверка работоспособности API
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 15s
  
  olap-db:
    # ClickHouse для OLAP-аналитики (данные импортируются из CRM и Telemetry БД)
    container_name: olap-db
    image: clickhouse/clickhouse-server:latest
    environment:
      CLICKHOUSE_DB: default  # База данных по умолчанию
      CLICKHOUSE_USER: default  # Пользователь по умолчанию
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1  # Разрешить управление доступом
      CLICKHOUSE_PASSWORD: "clickhouse_password"  # Пароль для разработки
    ports:
      - "8123:8123"  # HTTP-интерфейс для clickhouse-connect
      - "8443:8443"
      - "9431:9000"  # Native-протокол
    volumes:
#      - ./clickhouse-data:/var/lib/clickhouse  # Персистентное хранилище данных
      - ./clickhouse_config/users.d:/etc/clickhouse-server/users.d  # Конфигурация пользователей
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --password clickhouse_password --query 'SELECT 1'"]
      interval: 1s
      timeout: 3s
      retries: 30
    depends_on:
      debezium:
        condition: service_healthy  # Ждём готовности Debezium (чтобы данные начали поступать)
  zookeeper:
    # Zookeeper для координации Kafka
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181  # Порт для клиентов Zookeeper
      ZOOKEEPER_TICK_TIME: 2000  # Базовый интервал времени в миллисекундах
    ports:
      - "2181:2181"  # Порт Zookeeper
  
  kafka:
    # Kafka-брокер для потоковой передачи данных
    container_name: kafka
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper  # Kafka зависит от Zookeeper
    environment:
      KAFKA_BROKER_ID: 1  # Уникальный ID брокера
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  # Подключение к Zookeeper
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092  # Адреса для клиентов
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT  # Протоколы безопасности
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT  # Listener для межброкерного общения
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  # Фактор репликации для топика оффсетов
    ports:
      - "29092:29092"  # Порт Kafka для внешних подключений
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1"]  # Проверка готовности Kafka
      interval: 5s
      timeout: 10s
      retries: 10
      start_period: 30s
  
  kafdrop:
    # Kafdrop - веб-интерфейс для просмотра Kafka-топиков
    container_name: kafdrop
    image: obsidiandynamics/kafdrop:latest
    depends_on:
      - kafka  # Kafdrop зависит от Kafka
    environment:
      KAFKA_BROKERCONNECT: kafka:9092  # Подключение к Kafka-брокеру
      JVM_OPTS: "-Xms32M -Xmx64M"  # Настройки JVM для экономии памяти
    ports:
      - "9100:9000"  # Веб-интерфейс Kafdrop
  
  debezium:
    # Debezium Kafka Connect для CDC (Change Data Capture) из PostgreSQL
    # Используем кастомный образ с автоматической инициализацией коннекторов
    container_name: debezium
    build:
      context: ./debezium  # Папка с Dockerfile и скриптом инициализации
      dockerfile: Dockerfile  # Dockerfile для сборки кастомного образа
    depends_on:
      kafka:
        condition: service_healthy  # Ждём готовности Kafka
      crm-db:
        condition: service_healthy  # Ждём готовности CRM DB
      telemetry-db:
        condition: service_healthy  # Ждём готовности Telemetry DB
      crm-api:
        condition: service_healthy  # Ждём готовности CRM API
      telemetry-api:
        condition: service_healthy  # Ждём готовности Telemetry API
    environment:
      BOOTSTRAP_SERVERS: kafka:9092  # Адрес Kafka-брокера
      GROUP_ID: debezium-group  # ID группы для Kafka Connect
      CONFIG_STORAGE_TOPIC: debezium_configs  # Топик для хранения конфигураций
      OFFSET_STORAGE_TOPIC: debezium_offsets  # Топик для хранения оффсетов
      STATUS_STORAGE_TOPIC: debezium_statuses  # Топик для хранения статусов
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter  # Конвертер ключей
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter  # Конвертер значений
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"  # Отключаем схемы для ключей
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"  # Отключаем схемы для значений
    ports:
      - "8083:8083"  # REST API Kafka Connect
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/ || exit 1"]  # Проверка готовности Debezium
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 30s
  
  minio:
    container_name: minio
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
#    volumes:
#      - minio_storage:/data
    environment:
      MINIO_ROOT_USER: minio_user
      MINIO_ROOT_PASSWORD: minio_password
    command: server --console-address ":9001" /data
  
  bionicpro-frontend:
    # Фронтенд-приложение на React с интеграцией Keycloak
    container_name: bionicpro-frontend
    build:
      context: ./bionicpro_frontend  # Папка с Dockerfile и кодом фронтенда
      dockerfile: Dockerfile  # Dockerfile для сборки образа
    depends_on:
      keycloak:
        condition: service_healthy  # Ждём готовности Keycloak
    environment:
      # Переменные окружения для фронтенда (если нужны)
      VITE_KEYCLOAK_URL: http://localhost:8080  # URL Keycloak для браузера
      VITE_KEYCLOAK_REALM: reports-realm  # Realm Keycloak
      VITE_KEYCLOAK_CLIENT_ID: reports-frontend  # Client ID
    ports:
      - "5173:5173"  # Порт для доступа к фронтенду с хоста
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5173/ || exit 1"]  # Проверка работоспособности фронтенда
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 15s
  
  auth-proxy:
    # Прокси-сервис для авторизации с OIDC и управлением сессиями
    container_name: auth-proxy
    build:
      context: ./auth_proxy  # Папка с Dockerfile и кодом auth_proxy
      dockerfile: Dockerfile  # Dockerfile для сборки образа
    depends_on:
      bionicpro-frontend:
        condition: service_healthy  # Ждём готовности фронтенда
      keycloak:
        condition: service_healthy  # Ждём готовности Keycloak
      reports-api:
        condition: service_healthy  # Ждём готовности Reports API
      redis:
        condition: service_started  # Ждём запуска Redis
    environment:
      # Переменные окружения для auth_proxy
      AUTH_PROXY_KEYCLOAK_URL: http://keycloak:8080  # URL Keycloak внутри Docker-сети (для server-to-server)
      AUTH_PROXY_KEYCLOAK_PUBLIC_URL: http://localhost:8080  # Публичный URL Keycloak для браузера
      AUTH_PROXY_KEYCLOAK_REALM: reports-realm  # Realm Keycloak
      AUTH_PROXY_CLIENT_ID: reports-frontend  # Client ID
      AUTH_PROXY_CLIENT_SECRET: ""  # Client secret (пустой для public client)
      AUTH_PROXY_REDIS_HOST: redis  # Хост Redis (имя сервиса в Docker Compose)
      AUTH_PROXY_REDIS_PORT: 6379  # Порт Redis
      AUTH_PROXY_REDIS_PASSWORD: ""  # Пароль Redis (если установлен)
      AUTH_PROXY_FRONTEND_URL: http://bionicpro-frontend:5173  # Внутренний URL фронтенда (имя сервиса Docker)
      AUTH_PROXY_FRONTEND_PUBLIC_URL: http://localhost:3000  # URL фронтенда для браузера (через auth_proxy)
      AUTH_PROXY_SESSION_SECRET_KEY: "your-secret-key-change-in-production"  # Секретный ключ для сессий
      AUTH_PROXY_ENCRYPTION_KEY: ""  # Ключ шифрования токенов (опционально)
    ports:
      - "3000:3000"  # Порт для доступа к auth_proxy с хоста
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]  # Проверка работоспособности auth_proxy
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 15s

#volumes:
#  minio_storage: {}